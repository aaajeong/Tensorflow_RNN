# Tensorflow_RNN
Tensorflow RNN 

Tensorflow 을 이용한 RNN 공부 & 예제 실습



### 1. DeepLearningToAll (모두를 위한 딥러닝) - RNN

---

참고 : https://github.com/hunkim/DeepLearningZeroToAll

범위 : 시즌1 딥러닝의 기본 - Recurrent Neural Network 실습1 ~ 실습6



### 2. Tensorflow 을 이용한 영단어 번역 예제

---

참고 : https://woolulu.tistory.com/131

코드 : [Word_Translate.py](https://github.com/aaajeong/Tensorflow_RNN/blob/main/Word_Translate/Word_Translate%202.py) 

- Seq2Seq 을 활용

- 각 문자를 문자와 인덱스 번호로 딕셔녀리 형태를 만든 다음에, 예측 단계에서는 각 문자 별 확률 값에 따라 가장 큰 값을 가지는 문자를 출력해 최종 결과를 내고 있었습니다. (tf.argmax)

- 총 45 개의 character 중 가장 큰 값을 가진 인덱스에 해당하는 문자를 출력.

  ![](./img/seq_data.png)

```python
num_dic: {'S': 0, 'E': 1, 'P': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '단': 29, '어': 30, '나': 31, '무': 32, '놀': 33, '이': 34, '소': 35, '녀': 36, '키': 37, '스': 38, '사': 39, '랑': 40, '봉': 41, '구': 42, '우': 43, '루': 44}

```

1. 'Word' 를 '단어' 라고 **잘** 예측한 경우.

![](./img/word_result.png)

```python
int_value_model:  [[[-2 -3 -2 -4 -1 -3  0 -2 -2 -1 -2 -2 -1 -2  0 -4 -4 -6  0 -2 -2 -2 -2 -2 -4 -4 -1 -2  0 10 -4  3  3 -1  0  0 -2 -1 -4 -1 -1  0 -1  0  0] 
 	              [-6 -2 -3 -3 -3 -3 -2 -4 -1 -2 -1 -2 -1 -1 -3  0 -2 -3  0 -2 -1 -2 -4  -3 -1 -4 -3 -1 -3 -4 14 -3  4  1  0 -3 -1 -4  3 -1  2 -4  2  3  0] 
 	              [ 0 13  1  0 -1 -1 -2  0 -1  0  1  0 -1 -1 -1  1  0  2  0 -1  0 -4  0  -1  0  1  0  0  0 -2  0  0 -4 -2 -1 -1  1  0  1 -3  0 -2 -1 -1 -2]
                                     [ 0 -5 -1  1  1  0  1  2 -2  0 -2 -1  0  0  0  0 -1  0  3  0  0 -1  0  1  0 -1  1  1  0  3  0  1  1  1  0  0  0 -2 -3  1 -1  1 -2  2  1]
                                     [-1  0 -2  0  0  0  0  1  0  0  1  0  0 -3 -1  0 -1  0  0  0  0  0  0  0 -2  0  0  1  0 -1  0  1  1 -2  6 -3  1  0 -4  0  2 -2 -1 -2  0]]]

```

```
29번째 인덱스 값 : 10(단)
30번째 인덱스 값 : 14(어)
```



2. 'Wodr' 을 '나무' 라고 **잘못** 예측한 경우

![](./img/wodr_result.png)

```python
int_value_model:  [[[ 0  0 -1 -2 -2 -2  0 -4  0  0 -1  0 -1 -1 -1 -3  0 -1  0 -2 -1 -1 -2  0  0 -1 -1 -1 -1  5  1  8  5  0  1 -2 -3  0 -6  1  1 -1  0 -3 -1] 
  [-2  0 -1  1 -2  0  0 -2 -4 -2 -1 -2  0 -1 -2 -2  0 -1 -1 -2  0  0 -3  -2 -1 -1 -2  0 -2  2  6 -2  8  1  1  0  0 -2  3 -6  1 -2  4 -1 -2] 
  [-3 15 -7 -3 -3 -1 -2 -2 -1 -2 -2 -4  2  0 -1  0 -1 -4 -2 -2  0 -1 -3 -3 -1 -3 -1 -1 -1 -4  4  0 -4 -3  2  0  5 -2 -2 -2  1 -1 -2  2  0]
  [ 1  0  0  0  1  1 -1  0 -1  0 -1  2  0  0 -1  2  0 -2 -1  0 -2 -2  0 -1  0  0  0 -2  0  6  1  2  1  3  3  6  0 -2  0 -1 -7 -2 -5  2 -1]
  [-1  0 -2 -1 -1 -1 -2 -3 -2  0 -3  0 -4 -2  0  0 -1 -3 -2 -3 -4 -2 -3  -1  0 -2  0  0 -2  5  5  0  0 -3  6 -2  7 -1 -3  0  1 -4 -2 -3  6]]]

```

```
31번째 인덱스 값 : 8 (나)
32번째 인덱스 값 : 8 (무)
----------------------
29번째 인덱스 값 : 5 (단)
30번째 인덱스 값 : 6 (어)
```

3. Uncertainty 와 관련지어 생각해보기

- **Wodr** 이 word(단어), wood(나무) 와 비슷해서, *단어*, *나무* 라고 나올거라고 예측했는데, 실제로 argmax 하기 전의 값을 출력해보니, (나무), (단어)에 해당하는 인덱스에 해당하는 값들의 차이가 별로 나지 않는다는 것을 확인했다.
- 내가 생각하는 RNN 결과에 대한 **Candidate**
  - 다른 인덱스 보다 상대적으로 높은 점수를 가지고 있는 → '나무', '단어'
- Candidate 들 중, **Majority/Minority** 를 선택하는 과정에 대해서는 더 살펴봐야 할 것 같다.

4. 결론

   RNN 의 번역 예제에서도 CNN 과 같이 Uncertainty 을 적용할 수 있다는 점을 확인할 수 있었다. 더 깊은 이해를 위해서, 더 복잡한 모델을 사용한 예제를 통해 Uncertainty 을 파악해보면 좋을 것 같다.